{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.imports import *\n",
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = os.path.abspath('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labeled data (excluding test)\n",
    "label_csv = f'{PATH}/catalogs/SDSSspecgalsDR14_boada.csv'\n",
    "\n",
    "n = len(list(open(label_csv))) - 1 \n",
    "val_idxs = get_cv_idxs(n)\n",
    "\n",
    "# see a few\n",
    "df = pd.read_csv(label_csv, index_col='objID')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_num = 1237657070629027993\n",
    "os.path.isfile(f'{PATH}/images/{id_num}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine some of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ids = [os.path.split(fname)[1].strip('.png') for fname in glob(f'{PATH}/images/*.jpg')]\n",
    "image_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for obj in df.index[:5]:\n",
    "    display(PIL.Image.open(f'{PATH}/images/{obj}.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine distributions of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['nii_6584_flux', 'h_alpha_flux', 'oiii_5007_flux', 'h_beta_flux', 'h_delta_flux', \n",
    "           'd4000', 'bptclass', 'oh_p50', 'lgm_tot_p50', 'sfr_tot_p50']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=len(classes) // 2 + 1, figsize=(12, 6), sharey=True)\n",
    "\n",
    "for ax, col in zip(axes.flat, classes):\n",
    "    data = df[col]\n",
    "    ax.hist(data, range=np.nanpercentile(data, [5, 95]))\n",
    "    ax.set_xlabel(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's flag `bptclass` and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a mini training and test sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I executed the code below to copy a bunch of images to a training set (~10000) and a test set (~5000).\n",
    "\n",
    "```python\n",
    "# make a mini training sample of ~15000 images\n",
    "split_idxs = get_cv_idxs(n, val_pct=20000 / n) \n",
    "\n",
    "train_idxs = split_idxs[:-5000]\n",
    "test_idxs  = split_idxs[-5000:]\n",
    "\n",
    "# copy files to train-small dir, also make copy of data frame which only has \n",
    "# valid images\n",
    "valid_train_idxs = []\n",
    "for objid, idx in tqdm_notebook(zip(df.iloc[train_idxs].index, train_idxs), total=len(train_idxs)):\n",
    "    try:\n",
    "        shutil.copyfile(f'{PATH}/images/{objid}.jpg', f'{PATH}/train-small/{objid}.jpg')\n",
    "        valid_train_idxs.append(idx)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# save mini-dataframe\n",
    "df_train_small = df.iloc[valid_train_idxs].copy()\n",
    "df_train_small.to_csv(f'{PATH}/catalogs/train-small.csv')\n",
    "\n",
    "# do the same thing, except for test-small dataset\n",
    "valid_test_idxs = []\n",
    "for objid, idx in tqdm_notebook(zip(df.iloc[test_idxs].index, test_idxs), total=len(test_idxs)):\n",
    "    try:\n",
    "        shutil.copyfile(f'{PATH}/images/{objid}.jpg', f'{PATH}/test-small/{objid}.jpg')\n",
    "        valid_test_idxs.append(idx)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# save mini-dataframe\n",
    "df_test_small = df.iloc[valid_test_idxs].copy()\n",
    "df_test_small.to_csv(f'{PATH}/catalogs/test-small.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt the dataloader for using continuous variable output\n",
    "Here I used some helper functions defined by @farlion from the fast.ai forums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_multi_class_values(path_to_csv):\n",
    "    \"\"\"Parse filenames and values for classes from a CSV file.\n",
    "\n",
    "    This method expects that the csv file at path :fn: has one column for filenames,\n",
    "    while all the other columns represent classes.\n",
    "    Expects a header with class names\n",
    "\n",
    "    Arguments:\n",
    "        path_to_csv: Path to a CSV file.\n",
    "\n",
    "    Returns:\n",
    "        a three-tuple of:            \n",
    "            a list of filenames\n",
    "            a list of values in the same order\n",
    "            a dictionary of classes by classIndex           \n",
    "    \"\"\"\n",
    "    with open(path_to_csv) as fileobj:\n",
    "        reader = csv.reader(fileobj)\n",
    "        header = next(reader)\n",
    "        csv_lines = [l for l in reader]\n",
    "\n",
    "    fnames = [fname for fname, *_ in csv_lines]\n",
    "    classes = header[1:]\n",
    "    values = [vals for _, *vals in csv_lines]\n",
    "    idx2class = {i:c for i, c in enumerate(classes)}\n",
    "   \n",
    "    return fnames, values, idx2class\n",
    "\n",
    "def csv_source_multi_class(folder, csv_file, suffix=''):\n",
    "    fnames, values, idx2class = parse_csv_multi_class_values(csv_file)\n",
    "    full_names = [os.path.join(folder,fn+suffix) for fn in fnames]\n",
    "   \n",
    "    val_arr = np.array(values).astype(np.float32)\n",
    "    \n",
    "    return full_names, val_arr, idx2class\n",
    "\n",
    "@classmethod\n",
    "def from_multiclass_csv(cls, path, folder, csv_fname, bs=64, tfms=(None,None),\n",
    "           val_idxs=None, suffix='', test_name=None, num_workers=8):\n",
    "    \"\"\" Read in images and their labels given as a CSV file.\n",
    "--\n",
    "    This method should be used when training image labels are given in an CSV file as opposed to\n",
    "    sub-directories with label names.\n",
    "\n",
    "    Arguments:\n",
    "        path: a root path of the data (used for storing trained models, precomputed values, etc)\n",
    "        folder: a name of the folder in which training images are contained.\n",
    "        csv_fname: a name of the CSV file which contains target labels.\n",
    "        bs: batch size\n",
    "        tfms: transformations (for data augmentations). e.g. output of `tfms_from_model`\n",
    "        val_idxs: index of images to be used for validation. e.g. output of `get_cv_idxs`.\n",
    "            If None, default arguments to get_cv_idxs are used.\n",
    "        suffix: suffix to add to image names in CSV file (sometimes CSV only contains the file name without file\n",
    "                extension e.g. '.jpg' - in which case, you can set suffix as '.jpg')\n",
    "        test_name: a name of the folder which contains test images.\n",
    "        skip_header: skip the first row of the CSV file.\n",
    "        num_workers: number of workers\n",
    "\n",
    "    Returns:\n",
    "        ImageClassifierData\n",
    "    \"\"\"\n",
    "    fnames,y,idx2class = csv_source_multi_class(folder, csv_fname, suffix)\n",
    "\n",
    "    val_idxs = get_cv_idxs(len(fnames)) if val_idxs is None else val_idxs\n",
    "    ((val_fnames,trn_fnames),(val_y,trn_y)) = split_by_idx(val_idxs, np.array(fnames), y)\n",
    "\n",
    "    test_fnames = read_dir(path, test_name) if test_name else None\n",
    "    \n",
    "    f = FilesIndexArrayRegressionDataset\n",
    "    datasets = cls.get_ds(f, (trn_fnames,trn_y), (val_fnames,val_y), tfms,\n",
    "                           path=path, test=test_fnames)\n",
    "    return cls(path, datasets, bs, num_workers, classes=list(idx2class.values()))\n",
    "\n",
    "ImageClassifierData.from_multiclass_csv = from_multiclass_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a simple network to predict `oh_p50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_csv = f'{PATH}/catalogs/metallicity-train-small.csv'\n",
    "test_label_csv = f'{PATH}/catalogs/metallicity-test-small.csv'\n",
    "\n",
    "#df_train_small[['oh_p50']].to_csv(train_label_csv)\n",
    "#df_test_small[['oh_p50']].to_csv(test_label_csv)\n",
    "\n",
    "df_train_small = pd.read_csv(train_label_csv, index_col='objID')\n",
    "df_test_small = pd.read_csv(test_label_csv, index_col='objID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data with transforms\n",
    "arch = resnet34\n",
    "sz = 32\n",
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idxs = get_cv_idxs(len(list(open(train_label_csv))) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(sz, bs):\n",
    "    tfms = tfms_from_model(arch, sz, aug_tfms=transforms_top_down, max_zoom=1.1)\n",
    "    return ImageClassifierData.from_multiclass_csv(PATH, 'train-small', train_label_csv, tfms=tfms,\n",
    "                    suffix='.jpg', val_idxs=val_idxs, test_name='test-small', \n",
    "                    num_workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(sz, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~It appears that a batch size of `16` pushes my RAM to the limits (maybe even triggering swap space?), but I don't get an out of memory error -- except when lots of apps are open.~~ Now that I'm using categorical morphology, this seems to be going much more quickly and is light on my memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize network\n",
    "learn = ConvLearner.pretrained(arch, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5 \n",
    "print(learn.data.trn_y[0])\n",
    "PIL.Image.open(PATH + '/' + learn.data.trn_ds.fnames[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find learning rate\n",
    "lrf=learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start fitting the los-res images using `rmse` metric\n",
    "I'm using a learning rate of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(x, y):\n",
    "    return torch.sqrt(F.mse_loss(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(arch, data)\n",
    "metrics = [rmse]\n",
    "\n",
    "learn.crit = rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "learn.fit(lr, 3, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{sz}-small_init-train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like loss still has a little ways to go, but we'll move on. Before training on bigger images, let's unfreeze the earlier layers and train them too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differential learning rates\n",
    "learn.unfreeze()\n",
    "\n",
    "lrs = np.array([1/100, 1/10, 1]) * lr\n",
    "learn.fit(lrs, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{sz}-small_diff-learn-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the look of it, we're still not overfitting yet. In fact, the training `rmse` is *higher* than the crossval `rmse`... Does that mean that we're nowhere near overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 3, cycle_len=1, cycle_mult=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'{sz}-small_diff-learn-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining predictions using the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(f'{sz}-small_diff-learn-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.set_data(get_data(sz, bs))\n",
    "pred = learn.predict()\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation data examples')\n",
    "for i in range(5):\n",
    "    p, p_val = pred[i, 0], learn.data.val_y[i, 0]\n",
    "    print('Prediction: {:.3f}, True: {:.3f}, error: {:.3f}'.format(p, p_val, np.abs(p-p_val)))\n",
    "    display(PIL.Image.open(PATH + '/' + learn.data.val_ds.fnames[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check test set error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_test = learn.predict(is_test=True)\n",
    "#y_test = pd.read_csv(test_label_csv).oh_p50\n",
    "\n",
    "plt.hist((pred_test[:, 0] - y_test), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, let's see if we can do better by upgrading the image sizes..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuing using larger image sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doubling up from 32\n",
    "sz = 64\n",
    "bs = 64\n",
    "\n",
    "learn.set_data(get_data(sz, bs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stick with 0.1 I guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "learn.freeze()\n",
    "learn.fit(lr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unfreeze early layers again\n",
    "\n",
    "It looks like training is pretty slow so we'll want to zip things along..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "\n",
    "lrs = np.array([1/9, 1/3, 1]) * lr\n",
    "learn.fit(lrs, n_cycle=3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'32-64-small_diff-learn-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for a really long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, n_cycle=5, cycle_len=2, cycle_mult=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(f'32-64-small_diff-learn-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test, _ = learn.TTA(is_test=True)\n",
    "y_test = pd.read_csv(test_label_csv).oh_p50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = np.mean(pred_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((pred_test[:, 0] - y_test), bins=50);\n",
    "print('rmse = {:.3f}'.format(np.sqrt(np.mean((pred_test[:, 0] - y_test)**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test examples')\n",
    "for i in range(10):\n",
    "    p = pred_test[i, 0]\n",
    "    y = y_test[i]\n",
    "    print('Prediction: {:.3f}, True: {:.3f}, error: {:.3f}'.format(p, y, np.abs(p-y)))\n",
    "    display(PIL.Image.open(PATH + '/' + learn.data.test_ds.fnames[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And here's the validation distribution and examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = learn.predict()\n",
    "y_val = learn.data.val_y\n",
    "\n",
    "plt.hist(pred_val[:, 0] - y_val[:, 0], bins=50, range=[-0.5, 0.5])\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.xlabel('Validation: O/H p50 rmse [dex]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Validation examples')\n",
    "for i in range(10):\n",
    "    p = pred_val[i, 0]\n",
    "    y = y_val[i, 0]\n",
    "    print('Prediction: {:.3f}, True: {:.3f}, error: {:.3f}'.format(p, y, np.abs(p-y)))\n",
    "    display(PIL.Image.open(PATH + '/' + learn.data.trn_ds.fnames[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verdict: valdiation set is overfitting but the test set is still underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting the training, validation, and test sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_small.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_small.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 5, figsize=(12, 9))\n",
    "\n",
    "details = ['ra', 'dec', 'z']\n",
    "classes = ['nii_6584_flux', 'h_alpha_flux', 'oiii_5007_flux', 'h_beta_flux', 'h_delta_flux', \n",
    "           'd4000', 'bptclass', 'oh_p50', 'lgm_tot_p50', 'sfr_tot_p50']\n",
    "\n",
    "for ax, col in zip(axes.flat, details+classes):\n",
    "    ax.set_xlabel(col)\n",
    "    ax.hist(df_train_small[df_train_small.modelMag_r < 21.0][col], histtype='bar', bins=30)\n",
    "    ax.hist(df_test_small[df_test_small.modelMag_r < 21.0][col], histtype='bar', bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, aside from the quantities that are limited in dynamic range I think that we are okay here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if test indices are misaligned in the data loader?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in get_cv_idxs(len(df_test_small), val_pct=5 / len(df_test_small)):\n",
    "    img_name = data.test_ds.fnames[idx]\n",
    "    img_base = np.int64(os.path.splitext(os.path.basename(img_name))[0])\n",
    "    img = PIL.Image.open(PATH + '/' + img_name)\n",
    "    display(img)\n",
    "    \n",
    "    print(img_base)\n",
    "    print('Prediction (dataset): {:.3f}'.format(pred_test[idx, 0]))\n",
    "    print('Truth: {:.3f}'.format( df_test_small.loc[img_base].oh_p50))\n",
    "    print('Indexed: {:.3f}'.format(df_test_small.iloc[idx].oh_p50))\n",
    "    print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looks like the test indices got misaligned from the dataframe's\n",
    "\n",
    "How did this happen? ~~I need to figure that out...~~ Turns out that the dataframe needs to be sorted by index (see section 1.4.3 below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = learn.predict(is_test=True)\n",
    "\n",
    "# reorder by image name\n",
    "img_names = [np.int64(os.path.splitext(os.path.basename(img_name))[0]) for img_name in data.test_ds.fnames]\n",
    "y_test = df_test_small.loc[img_names].oh_p50\n",
    "\n",
    "plt.hist((pred_test[:, 0] - y_test), bins=50, range=[-0.5, 0.5])\n",
    "plt.xlim(-0.5, 0.5)\n",
    "\n",
    "print('rmse = {:.4f}'.format(np.sqrt(np.mean((pred_test[:, 0] - y_test)**2))))\n",
    "plt.xlabel('Test: O/H p50 rmse [dex]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the order of the test_label_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_small.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = resnet34\n",
    "val_idxs = get_cv_idxs(len(df_train_small))\n",
    "data = get_data(64, 64)\n",
    "\n",
    "img_names = [np.int64(os.path.splitext(os.path.basename(img_name))[0]) for img_name in data.test_ds.fnames]\n",
    "img_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_test_small.index)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! What went wrong is that the test image filenames were loaded in alphanumeric order, whereas the dataframe was not. So we just need to sort the data frame by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_small.sort_index(inplace=True)\n",
    "df_test_small.to_csv(test_label_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(arch, data)\n",
    "learn.load('32-64-small_diff-learn-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test_small.oh_p50.as_matrix()\n",
    "pred_test = learn.predict(is_test=True)[:, 0]\n",
    "print('Test examples (again)')\n",
    "\n",
    "for i in range(10):\n",
    "    p = pred_test[i]\n",
    "    y = y_test[i]\n",
    "    print('Prediction: {:.3f}, True: {:.3f}, error: {:.3f}'.format(p, y, np.abs(p-y)))\n",
    "    display(PIL.Image.open(PATH + '/' + learn.data.test_ds.fnames[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(pred_test - y_test, bins=50, range=[-0.5, 0.5])\n",
    "plt.xlabel('Test O/H p50 [dex])');\n",
    "\n",
    "print(np.sqrt(np.mean((pred_test - y_test)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase batch size\n",
    "bs = 128\n",
    "sz = 32\n",
    "\n",
    "data = get_data(sz, bs)\n",
    "learn = ConvLearner.pretrained(arch, data)\n",
    "learn.crit = rmse\n",
    "\n",
    "\n",
    "\n",
    "# find learning rate again\n",
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "\n",
    "learn.fit(lr, n_cycle=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "\n",
    "lrs = np.array([1/9, 1/3, 1]) * lr\n",
    "learn.fit(lrs, n_cycle=3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = learn.predict(is_test=True)[:, 0]\n",
    "y_test = df_test_small.oh_p50.as_matrix()\n",
    "\n",
    "print('Test rmse = {:.4f}'.format(np.sqrt(np.mean((pred_test - y_test)**2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move up to 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.set_data(get_data(64, bs))\n",
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze()\n",
    "lr = 0.1\n",
    "learn.fit(lr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = lr * np.array([1/9, 1/3, 1])\n",
    "learn.unfreeze()\n",
    "learn.fit(lrs, n_cycle=3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, 5)\n",
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(lrs, n_cycle=3, cycle_len=1, cycle_mult=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps?\n",
    "\n",
    "- Perhaps we can make a pipeline moving from 32 -> 64 -> 128, with batchsize ~ 128? \n",
    "- We can also add more data (rather than ~10^4 galaxies, throw ~10^5 at it). \n",
    "- Another option is to use ResNet50 or Resnet101."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_csv = f'{PATH}/catalogs/train.csv'\n",
    "df = pd.read_csv(train_label_csv, index_col=0)\n",
    "\n",
    "val_idxs = get_cv_idxs(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Create test-train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# randomly do ~80/20 split\n",
    "split_idxs = np.arange(len(df))\n",
    "np.random.shuffle(split_idxs)\n",
    "\n",
    "train_idxs = split_idxs[:-25000]\n",
    "test_idxs  = split_idxs[-25000:]\n",
    "\n",
    "# copy files to train-small dir, also make copy of data frame which only has \n",
    "# valid images\n",
    "valid_train_idxs = []\n",
    "for objid, idx in tqdm_notebook(zip(df.iloc[train_idxs].index, train_idxs), total=len(train_idxs)):\n",
    "    try:\n",
    "        shutil.copyfile(f'{PATH}/images/{objid}.jpg', f'{PATH}/train/{objid}.jpg')\n",
    "        valid_train_idxs.append(idx)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# save mini-dataframe\n",
    "df_train = df.iloc[valid_train_idxs].copy()\n",
    "df_train.to_csv(f'{PATH}/catalogs/train.csv')\n",
    "\n",
    "# do the same thing, except for test-small dataset\n",
    "valid_test_idxs = []\n",
    "for objid, idx in tqdm_notebook(zip(df.iloc[test_idxs].index, test_idxs), total=len(test_idxs)):\n",
    "    try:\n",
    "        shutil.copyfile(f'{PATH}/images/{objid}.jpg', f'{PATH}/test/{objid}.jpg')\n",
    "        valid_test_idxs.append(idx)\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "# save mini-dataframe *and sort by index*\n",
    "df_test = df.iloc[valid_test_idxs].copy()\n",
    "df_test.sort_index(inplace=True)\n",
    "df_test.to_csv(f'{PATH}/catalogs/test.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 32\n",
    "bs = 64\n",
    "arch = resnet34\n",
    "\n",
    "def get_data(sz, bs):\n",
    "    tfms = tfms_from_model(arch, sz, aug_tfms=transforms_top_down, max_zoom=1.1)\n",
    "    return ImageClassifierData.from_multiclass_csv(PATH, 'images', train_label_csv, tfms=tfms,\n",
    "                    suffix='.jpg', val_idxs=val_idxs, test_name='test', num_workers=4, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(sz, bs)\n",
    "learn = ConvLearner.pretrained(arch, data)\n",
    "\n",
    "def rmse(x, y):\n",
    "    return torch.sqrt(F.mse_loss(x,y))\n",
    "\n",
    "metrics = [rmse]\n",
    "learn.crit = rmse\n",
    "\n",
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early training\n",
    "\n",
    "~~Let's be adventurous and select a high learning rate of 0.3~~ $\\leftarrow$ too high, that started to diverge.\n",
    "\n",
    "~~Let's try a learning rate of 3e-3.~~ $\\leftarrow$ too low, that took over 5 epochs to make it to RMSE = 0.10.\n",
    "\n",
    "Perhaps let's try lr=0.1 again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "learn.fit(lr, 5, cycle_len=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss(n_skip=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "\n",
    "# lower the rate a little\n",
    "lrs = 3e-2 * np.array([1/9, 1/3, 1])\n",
    "\n",
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('32_diff-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anneal more?\n",
    "lrs = 1e-2 * np.array([1/16, 1/4, 1])\n",
    "learn.fit(lrs, 3, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('32_diff-2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 64-size training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('32_diff-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(64, 128)\n",
    "learn.set_data(data)\n",
    "\n",
    "learn.freeze()\n",
    "\n",
    "lr = 0.1\n",
    "learn.fit(lr, 3, cycle_len=1, cycle_mult=2)\n",
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('32-64_init')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze and train more\n",
    "learn.unfreeze()\n",
    "\n",
    "lrs = 0.01 * np.array([1/25, 1/5, 1])\n",
    "\n",
    "learn.fit(lr, 5, cycle_len=1, cycle_mult=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save('32-64_diff-1')\n",
    "learn.load('32-64_diff-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check validation set using test-time augmentation (TTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp, y_val = learn.TTA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = np.mean(logp, axis=0)\n",
    "\n",
    "for i in range(10):\n",
    "    p = p_val[i, 0]\n",
    "    y = y_val[i, 0]\n",
    "    print('Prediction: {:.3f}, True: {:.3f}, error: {:.3f}'.format(p, y, np.abs(p-y)))\n",
    "    display(PIL.Image.open(PATH + '/' + learn.data.test_ds.fnames[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(p_val[:,0] - y_val[:,0], bins=50, range=[-0.5, 0.5]);\n",
    "\n",
    "print('Val accuracy (with TTA) is {:.3f}'.format(np.sqrt(np.mean((p_val[:,0] - y_val[:,0])**2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the \"eyes\" of the resnet, which objects have the lowest or highest metallicity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lowest metallicity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for [idx] in np.argsort(p_val, axis=0)[:10]:\n",
    "    p = p_val[idx, 0]\n",
    "    y = y_val[idx, 0]\n",
    "    print('Prediction: {:.3f}, True: {:.3f}, error: {:.3f}'.format(p, y, np.abs(p-y)))\n",
    "    display(PIL.Image.open(PATH + '/' + learn.data.test_ds.fnames[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for [idx] in np.argsort(p_val, axis=0)[:-10:-1]:\n",
    "    p = p_val[idx, 0]\n",
    "    y = y_val[idx, 0]\n",
    "    print('Prediction: {:.3f}, True: {:.3f}, error: {:.3f}'.format(p, y, np.abs(p-y)))\n",
    "    display(PIL.Image.open(PATH + '/' + learn.data.test_ds.fnames[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate using test dataset\n",
    "\n",
    "This takes a while because there are about 20000 test images! Thus I'm using `learn.predict` rather than `learn.TTA` -- even though the latter would yield slightly better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = learn.predict(is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label_csv = f'{PATH}/catalogs/test.csv'\n",
    "y_test = pd.read_csv(test_label_csv).oh_p50\n",
    "\n",
    "plt.hist((p_test[:, 0] - y_test), bins=50);\n",
    "print('Test (no TTA) rmse is = {:.3f}'.format(np.sqrt(np.mean((p_test[:, 0] - y_test)**2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
